# -*- mode: ruby -*-
# vi: set ft=ruby :

# These variables are kept for reference in scripts, but we're not using them for network configuration
control_ip = "192.168.58.10"

# Determine the maximum number of agents, and set their IP addresses
agents = { "agent86" => "192.168.58.86",
           "agent99" => "192.168.58.99" }

# This is sized so that a machine with 16 threads and 16GB will allocate at most
# ~3/4 of its resources to the cluster.
control_vcpu = "4" # Number of vCPUs in the VM
control_memory = "5192" # 5G of Memory (An extra gig because its the control-plane node)

agent_vcpu = "4" # Number of vCPUs in the VM
agent_memory = "4096" # 4G of Memory

is_darwin_arm64 = Vagrant::Util::Platform.architecture == "arm64" && Vagrant::Util::Platform.darwin?

static_ips = <<-SHELL
    sudo -i
    cat /vagrant_shared/extra.hosts >> /etc/hosts
    SHELL

debian_packages = <<-SHELL
    sudo -i
    # Install necessary System tools/libraries
    apt-get update
    DEBIAN_FRONTEND=noninteractive apt-get upgrade -yqq --no-install-recommends
    echo "Updated and Upgraded"
    DEBIAN_FRONTEND=noninteractive apt-get install -yqq --no-install-recommends \
                    curl \
                    ca-certificates \
                    open-iscsi \
                    wget \
                    unzip \
                    zstd
    SHELL

helm_install_script = <<-SHELL
    sudo -i
    curl https://baltocdn.com/helm/signing.asc | gpg --dearmor | sudo tee /usr/share/keyrings/helm.gpg > /dev/null
    echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/helm.gpg] https://baltocdn.com/helm/stable/debian/ all main" | tee /etc/apt/sources.list.d/helm-stable-debian.list
    apt-get update
    apt-get install helm
    SHELL

control_plane_script = <<-SHELL
    sudo -i
    # Read preconfiguration from shared directory and install it
    mkdir -p /etc/rancher/k3s/
    mkdir -p /var/lib/rancher/k3s/server/manifests/
    cp /vagrant_shared/k3s/server/manifests/* /var/lib/rancher/k3s/server/manifests/
    cp /vagrant_shared/k3s/registries.yaml /etc/rancher/k3s/
    # Install and start k3s server node
    # Extra parameters in INSTALL_K3S_EXEC variable because of
    # K3s picking up the wrong interface when starting the control-plane and agents
    # https://github.com/alexellis/k3sup/issues/306
    export INSTALL_K3S_EXEC="--bind-address=#{control_ip} --node-external-ip=#{control_ip} --flannel-iface=eth1"
    curl -sfL https://get.k3s.io | sh -
    echo "Sleeping for 5 seconds to wait for k3s to start"
    sleep 5
    # Export generated config into shared directory so we can use it outside the cluster.
    cp /var/lib/rancher/k3s/server/token /vagrant_shared/
    cp /etc/rancher/k3s/k3s.yaml /vagrant_shared/
    # Install kubetail for debugging
    curl -sfL https://github.com/johanhaleby/kubetail/archive/refs/tags/1.6.20.tar.gz | tar xz
    mv kubetail-*/kubetail /usr/local/bin
    rm -rf kubetail-*
    SHELL

agent_script = <<-SHELL
    sudo -i
    # Read preconfiguration from shared directory and install it
    mkdir -p /etc/rancher/k3s/
    cp /vagrant_shared/k3s/registries.yaml /etc/rancher/k3s/
    # Install and start k3s agent nodes
    export K3S_TOKEN_FILE=/vagrant_shared/token
    export K3S_URL=https://#{control_ip}:6443
    export INSTALL_K3S_EXEC="--flannel-iface=eth1"
    curl -sfL https://get.k3s.io | sh -
    SHELL

cert_manager_install_script = <<-SHELL
    # See: https://cert-manager.io/docs/installation/helm/
    sudo  -i
    export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
    helm repo add jetstack https://charts.jetstack.io
    helm repo update
    helm install cert-manager jetstack/cert-manager --namespace cert-manager --create-namespace --set installCRDs=true
    kubectl apply -f /vagrant_shared/k3s/cert-manager/self-signed-issuer.yaml
    SHELL

longhorn_install_script = <<-SHELL
    # See: https://docs.k3s.io/storage
    #      https://longhorn.io/docs/1.6.2/deploy/install/install-with-kubectl
    #      https://longhorn.io/docs/1.6.2/deploy/install/install-with-helm/
    sudo  -i
    export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
    helm repo add longhorn https://charts.longhorn.io
    helm repo update
    helm install longhorn longhorn/longhorn --namespace longhorn-system --create-namespace --version 1.6.2
    kubectl apply -f /vagrant_shared/k3s/longhorn/storage-classes.yaml
    kubectl apply -f /vagrant_shared/k3s/longhorn/ingress.yaml
    SHELL

local_path_provisioner_script = <<-SHELL
    # See: https://github.com/rancher/local-path-provisioner/blob/master/README.md
    sudo   -i
    kubectl apply -f https://raw.githubusercontent.com/rancher/local-path-provisioner/v0.0.26/deploy/local-path-storage.yaml
    SHELL

registry_script = <<-SHELL
    sudo -i
    kubectl create namespace registry
    kubectl apply -f /vagrant_shared/k3s/registry/registry-service.yaml
    kubectl apply -f /vagrant_shared/k3s/registry/registry-ui.yaml
    SHELL

monitoring_install_script = <<-SHELL
    # See: https://k3s.rocks/metrics/
    sudo -i
    export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
    helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
    helm repo add grafana https://grafana.github.io/helm-charts
    helm repo update
    helm install prometheus-stack --version 58.6.0 -f /vagrant_shared/k3s/grafana-prometheus/prometheus-values.yaml prometheus-community/kube-prometheus-stack --create-namespace --namespace monitoring --wait
    kubectl apply -f /vagrant_shared/k3s/grafana-prometheus/alert-manager-ingress.yaml
    kubectl apply -f /vagrant_shared/k3s/grafana-prometheus/prometheus-ingress.yaml
    kubectl apply -f /vagrant_shared/k3s/grafana-prometheus/grafana-ingress.yaml
    kubectl apply -f /vagrant_shared/k3s/grafana-prometheus/traefik-service-monitor.yaml
    kubectl apply -f /vagrant_shared/k3s/grafana-prometheus/traefik-dashboard.yaml
    helm install loki grafana/loki-stack --namespace monitoring
    SHELL

scylladb_install_script = <<-SHELL
    # See: https://github.com/scylladb/scylla-operator/blob/master/docs/source/helm.md
    sudo -i
    export KUBECONFIG=/etc/rancher/k3s/k3s.yaml
    helm repo add scylla https://scylla-operator-charts.storage.googleapis.com/stable
    helm repo update
    helm install scylla-operator scylla/scylla-operator --values /vagrant_shared/k3s/scylladb/values.operator.yaml  --create-namespace --namespace scylla-operator --wait
    # Disable the Scylla Manager for now, consumes resources and we aren't using it.
    # helm install scylla-manager scylla/scylla-manager --values /vagrant_shared/k3s/scylladb/values.manager.yaml --create-namespace --namespace scylla-manager
    helm install scylla scylla/scylla --values /vagrant_shared/k3s/scylladb/values.cluster.yaml --create-namespace --namespace scylla
    # kubectl apply -f /vagrant_shared/k3s/scylladb/load-balancer.yaml
    SHELL

update_sysctl_limits = <<-SHELL
    echo "fs.inotify.max_user_watches=65536" >> /etc/sysctl.conf
    echo "fs.inotify.max_user_instances=1024" >> /etc/sysctl.conf
    sysctl -p
    SHELL

Vagrant.configure("2") do |config|
  if is_darwin_arm64
    config.vm.box = "bento/ubuntu-20.04-arm64"
  else
    config.vm.box = "bento/debian-12"
  end

  config.vm.network "forwarded_port", guest: 6443, host: 6443

  config.vm.define "control", primary: true do |control|
    control.vm.network "private_network", ip: control_ip
    control.vm.hostname = "control"

    if is_darwin_arm64
      control.vm.synced_folder "./shared", "/vagrant_shared", type: "rsync"
      control.vm.provider "parallels" do |p|
        p.cpus = control_vcpu
        p.memory = control_memory
      end
    else
      control.vm.network "private_network", ip: control_ip
      control.vm.synced_folder "./shared", "/vagrant_shared"
      control.vm.provider "virtualbox" do |vb|
        vb.cpus = control_vcpu
        vb.memory = control_memory
      end
    end

    control.vm.provision "shell", inline: update_sysctl_limits
    control.vm.provision "shell", inline: static_ips
    control.vm.provision "shell", inline: debian_packages
    control.vm.provision "shell", inline: helm_install_script
    control.vm.provision "shell", inline: control_plane_script
    control.vm.provision "shell", inline: cert_manager_install_script
    # We use longhorn, so don't setup the local-path-provisioner
    # control.vm.provision "shell", inline: local_path_provisioner_script
    control.vm.provision "shell", inline: longhorn_install_script
    control.vm.provision "shell", inline: monitoring_install_script
    control.vm.provision "shell", inline: registry_script
    control.vm.provision "shell", inline: scylladb_install_script
  end

  agents.each do |agent_name, agent_ip|
    config.vm.define agent_name do |agent|
      agent.vm.hostname = agent_name

      if is_darwin_arm64
        agent.vm.synced_folder "./shared", "/vagrant_shared", type: "rsync"
        agent.vm.provider "qemu" do |qe|
          qe.cpus = agent_vcpu
          qe.memory = agent_memory
          qe.gui = false
        end
      else
        agent.vm.synced_folder "./shared", "/vagrant_shared"
        agent.vm.provider "virtualbox" do |vb|
          vb.cpus = agent_vcpu
          vb.memory = agent_memory
        end
      end

      agent.vm.provision "shell", inline: update_sysctl_limits
      agent.vm.provision "shell", inline: static_ips
      agent.vm.provision "shell", inline: debian_packages
      agent.vm.provision "shell", inline: agent_script
    end
  end
end